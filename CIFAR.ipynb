{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee57f237-207b-4e3d-be43-95b2ea9f57de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'image_modeling'...\n",
      "remote: Enumerating objects: 80, done.\u001b[K\n",
      "remote: Counting objects: 100% (80/80), done.\u001b[K\n",
      "remote: Compressing objects: 100% (59/59), done.\u001b[K\n",
      "remote: Total 80 (delta 33), reused 58 (delta 19), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (80/80), 17.26 KiB | 3.45 MiB/s, done.\n",
      "Resolving deltas: 100% (33/33), done.\n",
      "/workspace/image_modeling\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/anti-integral/image_modeling\n",
    "%cd ./image_modeling/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2f0f5a2-5525-4568-b334-787a8ad351e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -qq -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1429385-87a2-4e16-9ee5-082036957736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set memory fraction to avoid fragmentation\n",
    "!export PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128\n",
    "\n",
    "# Or add to your Python script:\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3d533b7-37cd-41f0-8ebc-e6a00c51ab11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing CIFAR-10 data...\n",
      "Files already downloaded and verified\n",
      "Loading pipeline components...:   0%|                     | 0/2 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading pipeline components...: 100%|█████████████| 2/2 [00:00<00:00, 40.76it/s]\n",
      "Processed batch 0/782\n",
      "Processed batch 10/782\n",
      "Processed batch 20/782\n",
      "Processed batch 30/782\n",
      "Processed batch 40/782\n",
      "Processed batch 50/782\n",
      "Processed batch 60/782\n",
      "Processed batch 70/782\n",
      "Processed batch 80/782\n",
      "Processed batch 90/782\n",
      "Processed batch 100/782\n",
      "Processed batch 110/782\n",
      "Processed batch 120/782\n",
      "Processed batch 130/782\n",
      "Processed batch 140/782\n",
      "Processed batch 150/782\n",
      "Processed batch 160/782\n",
      "Processed batch 170/782\n",
      "Processed batch 180/782\n",
      "Processed batch 190/782\n",
      "Processed batch 200/782\n",
      "Processed batch 210/782\n",
      "Processed batch 220/782\n",
      "Processed batch 230/782\n",
      "Processed batch 240/782\n",
      "Processed batch 250/782\n",
      "Processed batch 260/782\n",
      "Processed batch 270/782\n",
      "Processed batch 280/782\n",
      "Processed batch 290/782\n",
      "Processed batch 300/782\n",
      "Processed batch 310/782\n",
      "Processed batch 320/782\n",
      "Processed batch 330/782\n",
      "Processed batch 340/782\n",
      "Processed batch 350/782\n",
      "Processed batch 360/782\n",
      "Processed batch 370/782\n",
      "Processed batch 380/782\n",
      "Processed batch 390/782\n",
      "Processed batch 400/782\n",
      "Processed batch 410/782\n",
      "Processed batch 420/782\n",
      "Processed batch 430/782\n",
      "Processed batch 440/782\n",
      "Processed batch 450/782\n",
      "Processed batch 460/782\n",
      "Processed batch 470/782\n",
      "Processed batch 480/782\n",
      "Processed batch 490/782\n",
      "Processed batch 500/782\n",
      "Processed batch 510/782\n",
      "Processed batch 520/782\n",
      "Processed batch 530/782\n",
      "Processed batch 540/782\n",
      "Processed batch 550/782\n",
      "Processed batch 560/782\n",
      "Processed batch 570/782\n",
      "Processed batch 580/782\n",
      "Processed batch 590/782\n",
      "Processed batch 600/782\n",
      "Processed batch 610/782\n",
      "Processed batch 620/782\n",
      "Processed batch 630/782\n",
      "Processed batch 640/782\n",
      "Processed batch 650/782\n",
      "Processed batch 660/782\n",
      "Processed batch 670/782\n",
      "Processed batch 680/782\n",
      "Processed batch 690/782\n",
      "Processed batch 700/782\n",
      "Processed batch 710/782\n",
      "Processed batch 720/782\n",
      "Processed batch 730/782\n",
      "Processed batch 740/782\n",
      "Processed batch 750/782\n",
      "Processed batch 760/782\n",
      "Processed batch 770/782\n",
      "Processed batch 780/782\n",
      "Data processing complete!\n"
     ]
    }
   ],
   "source": [
    "!python main.py process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0079080-5500-484b-a4d8-e9ef8c80bc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for 100 epochs...\n",
      "Loading pipeline components...:   0%|                     | 0/2 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Loading pipeline components...: 100%|█████████████| 2/2 [00:00<00:00, 43.86it/s]\n",
      "Starting training for 100 epochs...\n",
      "Dataset size: 50000\n",
      "Batch size: 32\n",
      "Steps per epoch: 1563\n",
      "Epoch 1/100, Batch 0/1563, Loss: 21.211119\n",
      "Epoch 1/100, Batch 50/1563, Loss: 18.831404\n",
      "Epoch 1/100, Batch 100/1563, Loss: 16.600155\n",
      "Epoch 1/100, Batch 150/1563, Loss: 17.440311\n",
      "Epoch 1/100, Batch 200/1563, Loss: 17.814037\n",
      "Epoch 1/100, Batch 250/1563, Loss: 18.082119\n",
      "Epoch 1/100, Batch 300/1563, Loss: 18.363258\n",
      "Epoch 1/100, Batch 350/1563, Loss: 18.564571\n",
      "Epoch 1/100, Batch 400/1563, Loss: 18.717155\n",
      "Epoch 1/100, Batch 450/1563, Loss: 18.768810\n",
      "Epoch 1/100, Batch 500/1563, Loss: 18.831160\n",
      "Epoch 1/100, Batch 550/1563, Loss: 18.784550\n",
      "Epoch 1/100, Batch 600/1563, Loss: 18.677479\n",
      "Epoch 1/100, Batch 650/1563, Loss: 18.701803\n",
      "Epoch 1/100, Batch 700/1563, Loss: 18.525883\n",
      "Epoch 1/100, Batch 750/1563, Loss: 18.570007\n",
      "Epoch 1/100, Batch 800/1563, Loss: 18.471317\n",
      "Epoch 1/100, Batch 850/1563, Loss: 18.334890\n",
      "Epoch 1/100, Batch 900/1563, Loss: 18.225206\n",
      "Epoch 1/100, Batch 950/1563, Loss: 18.074387\n",
      "Epoch 1/100, Batch 1000/1563, Loss: 18.022964\n",
      "Epoch 1/100, Batch 1050/1563, Loss: 18.002766\n",
      "Epoch 1/100, Batch 1100/1563, Loss: 17.954494\n",
      "Epoch 1/100, Batch 1150/1563, Loss: 17.845476\n",
      "Epoch 1/100, Batch 1200/1563, Loss: 17.764160\n",
      "Epoch 1/100, Batch 1250/1563, Loss: 17.598564\n",
      "Epoch 1/100, Batch 1300/1563, Loss: 17.660259\n",
      "Epoch 1/100, Batch 1350/1563, Loss: 17.495838\n",
      "Epoch 1/100, Batch 1400/1563, Loss: 17.504911\n",
      "Epoch 1/100, Batch 1450/1563, Loss: 17.361687\n",
      "Epoch 1/100, Batch 1500/1563, Loss: 17.267715\n",
      "Epoch 1/100, Batch 1550/1563, Loss: 17.177984\n",
      "Epoch 1/100, Loss: 18.092656, Time: 38.8s, Total: 38.8s\n",
      "Epoch 2/100, Batch 0/1563, Loss: 17.174339\n",
      "Epoch 2/100, Batch 50/1563, Loss: 17.103168\n",
      "Epoch 2/100, Batch 100/1563, Loss: 16.961536\n",
      "Epoch 2/100, Batch 150/1563, Loss: 16.826277\n",
      "Epoch 2/100, Batch 200/1563, Loss: 16.718847\n",
      "Epoch 2/100, Batch 250/1563, Loss: 16.622036\n",
      "Epoch 2/100, Batch 300/1563, Loss: 16.474892\n",
      "Epoch 2/100, Batch 350/1563, Loss: 16.407879\n",
      "Epoch 2/100, Batch 400/1563, Loss: 16.260717\n",
      "Epoch 2/100, Batch 450/1563, Loss: 16.189360\n",
      "Epoch 2/100, Batch 500/1563, Loss: 16.185322\n",
      "Epoch 2/100, Batch 550/1563, Loss: 16.021986\n",
      "Epoch 2/100, Batch 600/1563, Loss: 15.936619\n",
      "Epoch 2/100, Batch 650/1563, Loss: 15.930495\n",
      "Epoch 2/100, Batch 700/1563, Loss: 15.889395\n",
      "Epoch 2/100, Batch 750/1563, Loss: 15.710047\n",
      "Epoch 2/100, Batch 800/1563, Loss: 15.654516\n",
      "Epoch 2/100, Batch 850/1563, Loss: 15.792216\n",
      "Epoch 2/100, Batch 900/1563, Loss: 15.882942\n",
      "Epoch 2/100, Batch 950/1563, Loss: 15.792789\n",
      "Epoch 2/100, Batch 1000/1563, Loss: 15.724781\n",
      "Epoch 2/100, Batch 1050/1563, Loss: 15.641478\n",
      "Epoch 2/100, Batch 1100/1563, Loss: 15.572842\n",
      "Epoch 2/100, Batch 1150/1563, Loss: 15.535372\n",
      "Epoch 2/100, Batch 1200/1563, Loss: 15.437265\n",
      "Epoch 2/100, Batch 1250/1563, Loss: 15.417038\n",
      "Epoch 2/100, Batch 1300/1563, Loss: 15.381117\n",
      "Epoch 2/100, Batch 1350/1563, Loss: 15.290239\n",
      "Epoch 2/100, Batch 1400/1563, Loss: 15.246729\n",
      "Epoch 2/100, Batch 1450/1563, Loss: 15.176800\n",
      "Epoch 2/100, Batch 1500/1563, Loss: 15.177246\n",
      "Epoch 2/100, Batch 1550/1563, Loss: 14.941447\n",
      "Epoch 2/100, Loss: 15.910724, Time: 37.6s, Total: 76.5s\n",
      "Epoch 3/100, Batch 0/1563, Loss: 15.188571\n",
      "Epoch 3/100, Batch 50/1563, Loss: 15.013400\n",
      "Epoch 3/100, Batch 100/1563, Loss: 15.003383\n",
      "Epoch 3/100, Batch 150/1563, Loss: 15.036062\n",
      "Epoch 3/100, Batch 200/1563, Loss: 15.002928\n",
      "Epoch 3/100, Batch 250/1563, Loss: 14.951866\n",
      "Epoch 3/100, Batch 300/1563, Loss: 14.890738\n",
      "Epoch 3/100, Batch 350/1563, Loss: 14.874836\n",
      "Epoch 3/100, Batch 400/1563, Loss: 14.863731\n",
      "Epoch 3/100, Batch 450/1563, Loss: 14.769897\n",
      "Epoch 3/100, Batch 500/1563, Loss: 14.726233\n",
      "Epoch 3/100, Batch 550/1563, Loss: 14.661934\n",
      "Epoch 3/100, Batch 600/1563, Loss: 14.633993\n",
      "Epoch 3/100, Batch 650/1563, Loss: 14.682284\n",
      "Epoch 3/100, Batch 700/1563, Loss: 14.695194\n",
      "Epoch 3/100, Batch 750/1563, Loss: 14.819091\n",
      "Epoch 3/100, Batch 800/1563, Loss: 14.746275\n",
      "Epoch 3/100, Batch 850/1563, Loss: 14.663032\n",
      "Epoch 3/100, Batch 900/1563, Loss: 14.695374\n",
      "Epoch 3/100, Batch 950/1563, Loss: 14.728867\n",
      "Epoch 3/100, Batch 1000/1563, Loss: 14.604855\n",
      "Epoch 3/100, Batch 1050/1563, Loss: 14.622425\n",
      "Epoch 3/100, Batch 1100/1563, Loss: 14.648642\n",
      "Epoch 3/100, Batch 1150/1563, Loss: 14.613754\n",
      "Epoch 3/100, Batch 1200/1563, Loss: 14.604631\n",
      "Epoch 3/100, Batch 1250/1563, Loss: 14.672657\n",
      "Epoch 3/100, Batch 1300/1563, Loss: 14.543796\n",
      "Epoch 3/100, Batch 1350/1563, Loss: 14.614433\n",
      "Epoch 3/100, Batch 1400/1563, Loss: 14.548343\n",
      "Epoch 3/100, Batch 1450/1563, Loss: 14.657265\n",
      "Epoch 3/100, Batch 1500/1563, Loss: 14.588432\n",
      "Epoch 3/100, Batch 1550/1563, Loss: 14.494926\n",
      "Epoch 3/100, Loss: 14.745868, Time: 35.7s, Total: 112.1s\n",
      "Epoch 4/100, Batch 0/1563, Loss: 14.535876\n",
      "Epoch 4/100, Batch 50/1563, Loss: 14.627968\n",
      "Epoch 4/100, Batch 100/1563, Loss: 14.640633\n",
      "Epoch 4/100, Batch 150/1563, Loss: 14.699034\n",
      "Epoch 4/100, Batch 200/1563, Loss: 14.532959\n",
      "Epoch 4/100, Batch 250/1563, Loss: 14.699191\n",
      "Epoch 4/100, Batch 300/1563, Loss: 14.597383\n",
      "Epoch 4/100, Batch 350/1563, Loss: 14.565195\n",
      "Epoch 4/100, Batch 400/1563, Loss: 14.539961\n",
      "Epoch 4/100, Batch 450/1563, Loss: 14.567419\n",
      "Epoch 4/100, Batch 500/1563, Loss: 14.634433\n",
      "Epoch 4/100, Batch 550/1563, Loss: 14.589552\n",
      "Epoch 4/100, Batch 600/1563, Loss: 14.655787\n",
      "Epoch 4/100, Batch 650/1563, Loss: 14.697436\n",
      "Epoch 4/100, Batch 700/1563, Loss: 14.559512\n",
      "Epoch 4/100, Batch 750/1563, Loss: 14.599269\n",
      "Epoch 4/100, Batch 800/1563, Loss: 14.463805\n",
      "Epoch 4/100, Batch 850/1563, Loss: 14.536506\n",
      "Epoch 4/100, Batch 900/1563, Loss: 14.556971\n",
      "Epoch 4/100, Batch 950/1563, Loss: 14.579844\n",
      "Epoch 4/100, Batch 1000/1563, Loss: 14.554190\n",
      "Epoch 4/100, Batch 1050/1563, Loss: 14.509509\n",
      "Epoch 4/100, Batch 1100/1563, Loss: 14.397840\n",
      "Epoch 4/100, Batch 1150/1563, Loss: 14.499057\n",
      "Epoch 4/100, Batch 1200/1563, Loss: 14.463034\n",
      "Epoch 4/100, Batch 1250/1563, Loss: 14.514641\n",
      "Epoch 4/100, Batch 1300/1563, Loss: 14.412799\n",
      "Epoch 4/100, Batch 1350/1563, Loss: 14.399058\n",
      "Epoch 4/100, Batch 1400/1563, Loss: 14.472251\n",
      "Epoch 4/100, Batch 1450/1563, Loss: 14.330783\n",
      "Epoch 4/100, Batch 1500/1563, Loss: 14.402098\n",
      "Epoch 4/100, Batch 1550/1563, Loss: 14.288330\n",
      "Epoch 4/100, Loss: 14.545203, Time: 37.5s, Total: 149.7s\n",
      "Epoch 5/100, Batch 0/1563, Loss: 14.315592\n",
      "Epoch 5/100, Batch 50/1563, Loss: 14.354274\n",
      "Epoch 5/100, Batch 100/1563, Loss: 14.306404\n",
      "Epoch 5/100, Batch 150/1563, Loss: 14.402777\n",
      "Epoch 5/100, Batch 200/1563, Loss: 14.330788\n",
      "Epoch 5/100, Batch 250/1563, Loss: 14.252161\n",
      "Epoch 5/100, Batch 300/1563, Loss: 14.347525\n",
      "Epoch 5/100, Batch 350/1563, Loss: 14.211867\n",
      "Epoch 5/100, Batch 400/1563, Loss: 14.153229\n",
      "Epoch 5/100, Batch 450/1563, Loss: 14.258154\n",
      "Epoch 5/100, Batch 500/1563, Loss: 14.184057\n",
      "Epoch 5/100, Batch 550/1563, Loss: 14.107121\n",
      "Epoch 5/100, Batch 600/1563, Loss: 14.213261\n",
      "Epoch 5/100, Batch 650/1563, Loss: 14.056209\n",
      "Epoch 5/100, Batch 700/1563, Loss: 14.140865\n",
      "Epoch 5/100, Batch 750/1563, Loss: 14.050468\n",
      "Epoch 5/100, Batch 800/1563, Loss: 13.937584\n",
      "Epoch 5/100, Batch 850/1563, Loss: 14.103388\n",
      "Epoch 5/100, Batch 900/1563, Loss: 14.055037\n",
      "Epoch 5/100, Batch 950/1563, Loss: 13.980804\n",
      "Epoch 5/100, Batch 1000/1563, Loss: 13.995879\n",
      "Epoch 5/100, Batch 1050/1563, Loss: 13.844363\n",
      "Epoch 5/100, Batch 1100/1563, Loss: 13.823870\n",
      "Epoch 5/100, Batch 1150/1563, Loss: 13.785605\n",
      "Epoch 5/100, Batch 1200/1563, Loss: 13.707220\n",
      "Epoch 5/100, Batch 1250/1563, Loss: 13.764206\n",
      "Epoch 5/100, Batch 1300/1563, Loss: 13.691603\n",
      "Epoch 5/100, Batch 1350/1563, Loss: 13.759265\n",
      "Epoch 5/100, Batch 1400/1563, Loss: 13.631354\n",
      "Epoch 5/100, Batch 1450/1563, Loss: 13.607077\n",
      "Epoch 5/100, Batch 1500/1563, Loss: 13.605372\n",
      "Epoch 5/100, Batch 1550/1563, Loss: 13.536070\n",
      "Epoch 5/100, Loss: 14.033777, Time: 36.6s, Total: 186.3s\n",
      "Epoch 6/100, Batch 0/1563, Loss: 13.622110\n",
      "Epoch 6/100, Batch 50/1563, Loss: 13.553410\n",
      "Epoch 6/100, Batch 100/1563, Loss: 13.558055\n",
      "Epoch 6/100, Batch 150/1563, Loss: 13.403540\n",
      "Epoch 6/100, Batch 200/1563, Loss: 13.378141\n",
      "Epoch 6/100, Batch 250/1563, Loss: 13.405154\n",
      "Epoch 6/100, Batch 300/1563, Loss: 13.485456\n",
      "Epoch 6/100, Batch 350/1563, Loss: 13.534933\n",
      "Epoch 6/100, Batch 400/1563, Loss: 13.337020\n",
      "Epoch 6/100, Batch 450/1563, Loss: 13.272596\n",
      "Epoch 6/100, Batch 500/1563, Loss: 13.294165\n",
      "Epoch 6/100, Batch 550/1563, Loss: 13.208281\n",
      "Epoch 6/100, Batch 600/1563, Loss: 13.144099\n",
      "Epoch 6/100, Batch 650/1563, Loss: 13.237437\n",
      "Epoch 6/100, Batch 700/1563, Loss: 13.207869\n",
      "Epoch 6/100, Batch 750/1563, Loss: 13.095622\n",
      "Epoch 6/100, Batch 800/1563, Loss: 13.109824\n",
      "Epoch 6/100, Batch 850/1563, Loss: 13.155238\n",
      "Epoch 6/100, Batch 900/1563, Loss: 13.085957\n",
      "Epoch 6/100, Batch 950/1563, Loss: 13.007747\n",
      "Epoch 6/100, Batch 1000/1563, Loss: 13.042865\n",
      "Epoch 6/100, Batch 1050/1563, Loss: 12.871058\n",
      "Epoch 6/100, Batch 1100/1563, Loss: 12.926237\n",
      "Epoch 6/100, Batch 1150/1563, Loss: 12.715595\n",
      "Epoch 6/100, Batch 1200/1563, Loss: 12.828588\n",
      "Epoch 6/100, Batch 1250/1563, Loss: 12.741011\n",
      "Epoch 6/100, Batch 1300/1563, Loss: 12.731628\n",
      "Epoch 6/100, Batch 1350/1563, Loss: 12.783072\n",
      "Epoch 6/100, Batch 1400/1563, Loss: 12.661128\n",
      "Epoch 6/100, Batch 1450/1563, Loss: 12.659798\n",
      "Epoch 6/100, Batch 1500/1563, Loss: 12.559821\n",
      "Epoch 6/100, Batch 1550/1563, Loss: 12.656177\n",
      "Epoch 6/100, Loss: 13.093769, Time: 36.6s, Total: 222.8s\n",
      "Epoch 7/100, Batch 0/1563, Loss: 12.544108\n",
      "Epoch 7/100, Batch 50/1563, Loss: 12.526945\n",
      "Epoch 7/100, Batch 100/1563, Loss: 12.369085\n",
      "Epoch 7/100, Batch 150/1563, Loss: 12.276225\n",
      "Epoch 7/100, Batch 200/1563, Loss: 12.371359\n",
      "Epoch 7/100, Batch 250/1563, Loss: 12.084323\n",
      "Epoch 7/100, Batch 300/1563, Loss: 12.204536\n",
      "Epoch 7/100, Batch 350/1563, Loss: 12.094559\n",
      "Epoch 7/100, Batch 400/1563, Loss: 12.061591\n",
      "Epoch 7/100, Batch 450/1563, Loss: 12.139720\n",
      "Epoch 7/100, Batch 500/1563, Loss: 12.052845\n",
      "Epoch 7/100, Batch 550/1563, Loss: 12.025425\n",
      "Epoch 7/100, Batch 600/1563, Loss: 11.934631\n",
      "Epoch 7/100, Batch 650/1563, Loss: 11.988831\n",
      "Epoch 7/100, Batch 700/1563, Loss: 11.879461\n",
      "Epoch 7/100, Batch 750/1563, Loss: 11.870640\n",
      "Epoch 7/100, Batch 800/1563, Loss: 11.715750\n",
      "Epoch 7/100, Batch 850/1563, Loss: 11.763594\n",
      "Epoch 7/100, Batch 900/1563, Loss: 11.640319\n",
      "Epoch 7/100, Batch 950/1563, Loss: 11.597588\n",
      "Epoch 7/100, Batch 1000/1563, Loss: 11.566646\n",
      "Epoch 7/100, Batch 1050/1563, Loss: 11.551252\n",
      "Epoch 7/100, Batch 1100/1563, Loss: 11.449752\n",
      "Epoch 7/100, Batch 1150/1563, Loss: 11.464848\n",
      "Epoch 7/100, Batch 1200/1563, Loss: 11.246647\n",
      "Epoch 7/100, Batch 1250/1563, Loss: 11.293777\n",
      "Epoch 7/100, Batch 1300/1563, Loss: 11.232524\n",
      "Epoch 7/100, Batch 1350/1563, Loss: 11.143551\n",
      "Epoch 7/100, Batch 1400/1563, Loss: 11.157951\n",
      "Epoch 7/100, Batch 1450/1563, Loss: 11.088336\n",
      "Epoch 7/100, Batch 1500/1563, Loss: 11.153076\n",
      "Epoch 7/100, Batch 1550/1563, Loss: 11.044056\n",
      "Epoch 7/100, Loss: 11.782936, Time: 37.2s, Total: 260.0s\n",
      "Epoch 8/100, Batch 0/1563, Loss: 10.951792\n",
      "Epoch 8/100, Batch 50/1563, Loss: 10.830391\n",
      "Epoch 8/100, Batch 100/1563, Loss: 10.907312\n",
      "Epoch 8/100, Batch 150/1563, Loss: 10.897998\n",
      "Epoch 8/100, Batch 200/1563, Loss: 10.833876\n",
      "Epoch 8/100, Batch 250/1563, Loss: 10.631754\n",
      "Epoch 8/100, Batch 300/1563, Loss: 11.018423\n",
      "Epoch 8/100, Batch 350/1563, Loss: 10.821418\n",
      "Epoch 8/100, Batch 400/1563, Loss: 10.762954\n",
      "Epoch 8/100, Batch 450/1563, Loss: 10.680153\n",
      "Epoch 8/100, Batch 500/1563, Loss: 10.596056\n",
      "Epoch 8/100, Batch 550/1563, Loss: 10.536222\n",
      "Epoch 8/100, Batch 600/1563, Loss: 10.555082\n",
      "Epoch 8/100, Batch 650/1563, Loss: 10.515936\n",
      "Epoch 8/100, Batch 700/1563, Loss: 10.469740\n",
      "Epoch 8/100, Batch 750/1563, Loss: 10.517362\n",
      "Epoch 8/100, Batch 800/1563, Loss: 10.427232\n",
      "Epoch 8/100, Batch 850/1563, Loss: 10.378057\n",
      "Epoch 8/100, Batch 900/1563, Loss: 10.387520\n",
      "Epoch 8/100, Batch 950/1563, Loss: 10.394161\n",
      "Epoch 8/100, Batch 1000/1563, Loss: 10.214281\n",
      "Epoch 8/100, Batch 1050/1563, Loss: 10.273677\n",
      "Epoch 8/100, Batch 1100/1563, Loss: 10.196982\n",
      "Epoch 8/100, Batch 1150/1563, Loss: 10.079017\n",
      "Epoch 8/100, Batch 1200/1563, Loss: 9.959047\n",
      "Epoch 8/100, Batch 1250/1563, Loss: 9.944552\n",
      "Epoch 8/100, Batch 1300/1563, Loss: 10.158323\n",
      "Epoch 8/100, Batch 1350/1563, Loss: 9.961832\n",
      "Epoch 8/100, Batch 1400/1563, Loss: 10.115980\n",
      "Epoch 8/100, Batch 1450/1563, Loss: 9.964083\n"
     ]
    }
   ],
   "source": [
    "!python main.py train --epochs 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e94ca6e6-0b74-49d6-a71d-a43407edfc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "88dfec93-c15c-4b76-bf23-82f3fead0f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model from checkpoints/best_model.pt...\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/image_modeling/main.py\", line 90, in <module>\n",
      "    main()\n",
      "  File \"/workspace/image_modeling/main.py\", line 86, in main\n",
      "    fid_score = evaluate_trained_model(args.checkpoint)\n",
      "  File \"/workspace/image_modeling/main.py\", line 46, in evaluate_trained_model\n",
      "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 986, in load\n",
      "    with _open_file_like(f, 'rb') as opened_file:\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 435, in _open_file_like\n",
      "    return _open_file(name_or_buffer, mode)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/serialization.py\", line 416, in __init__\n",
      "    super().__init__(open(name, mode))\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'checkpoints/best_model.pt'\n"
     ]
    }
   ],
   "source": [
    "!python main.py evaluate --model_path checkpoints/dsno_final.pth --num_samples 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "87f891e3-e980-45b1-8919-3098421d7661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch-fid in /usr/local/lib/python3.10/dist-packages (0.3.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pytorch-fid) (1.24.1)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from pytorch-fid) (9.3.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pytorch-fid) (1.15.3)\n",
      "Requirement already satisfied: torch>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from pytorch-fid) (2.1.0+cu118)\n",
      "Requirement already satisfied: torchvision>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from pytorch-fid) (0.16.0+cu118)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.1->pytorch-fid) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.1->pytorch-fid) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.1->pytorch-fid) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.1->pytorch-fid) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.1->pytorch-fid) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.1->pytorch-fid) (2025.5.1)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.1->pytorch-fid) (2.1.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision>=0.2.2->pytorch-fid) (2.31.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.1->pytorch-fid) (2.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.2.2->pytorch-fid) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.2.2->pytorch-fid) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.2.2->pytorch-fid) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision>=0.2.2->pytorch-fid) (2022.12.7)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.1->pytorch-fid) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_fid/__main__.py\", line 3, in <module>\n",
      "    pytorch_fid.fid_score.main()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_fid/fid_score.py\", line 313, in main\n",
      "    fid_value = calculate_fid_given_paths(args.path,\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_fid/fid_score.py\", line 253, in calculate_fid_given_paths\n",
      "    raise RuntimeError('Invalid path: %s' % p)\n",
      "RuntimeError: Invalid path: data/real_cifar10\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-fid\n",
    "!python -m pytorch_fid data/real_cifar10 evaluation_results/generated_images --device cuda"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
